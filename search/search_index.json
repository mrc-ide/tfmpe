{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TFMPE","text":"<p>Tokenised Flow Matching for Posterior Estimation</p> <p>TFMPE is a Python package for efficient posterior estimation using structured parameter representations and flow matching techniques. It provides modular tools for parameter estimation, data preprocessing, and neural network architectures optimized for scientific computing applications.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#basic-installation","title":"Basic Installation","text":"<p>Install the core package with default dependencies:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#development-installation","title":"Development Installation","text":"<p>For development work including testing and type checking:</p> <pre><code>pip install -e .[dev]\n</code></pre>"},{"location":"#examples-and-plotting","title":"Examples and Plotting","text":"<p>For running examples with visualization capabilities:</p> <pre><code>pip install -e .[examples]\n</code></pre>"},{"location":"#all-dependencies","title":"All Dependencies","text":"<p>Install everything for full development and examples:</p> <pre><code>pip install -e .[dev,examples]\n</code></pre>"},{"location":"#development-setup","title":"Development Setup","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python \u2265 3.10</li> </ul>"},{"location":"#testing","title":"Testing","text":"<p>The project uses pytest with custom markers for different test categories:</p> <pre><code># Run standard test suite (fast tests only)\npython -m pytest test/\n\n# Run all tests including slow ones\npython -m pytest test/ -m \"slow or not slow\"\n\n# Run only slow tests\npython -m pytest test/ -m \"slow\"\n\n# Run speed benchmarks\npython -m pytest test/ -m \"speed\"\n\n# Run scale benchmarks\npython -m pytest test/ -m \"scale\"\n\n# Run all tests (including benchmarks)\npython -m pytest test/ -m \"\"\n</code></pre>"},{"location":"#type-checking-and-linting","title":"Type checking and linting","text":"<p>Static type analysis: <code>pyright</code> Linting: <code>ruff</code></p>"},{"location":"#documentation","title":"Documentation","text":"<p>Build documentation locally:</p> <pre><code>pip install -e .[docs]\nmkdocs build\n</code></pre> <p>Serve documentation with live reload:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"#package-structure","title":"Package Structure","text":"<pre><code>tfmpe/\n\u251c\u2500\u2500 metrics/          # Metrics for analysis of parameter inference\n\u251c\u2500\u2500 estimators/       # Parameter estimators\n\u251c\u2500\u2500 bijectors/        # Bijectors for structured parameter sets\n\u251c\u2500\u2500 preprocessing/    # Pipelines for processing datasets\n\u251c\u2500\u2500 sampling/         # Sampling algorithms for training estimators\n\u2514\u2500\u2500 nn/              # Neural networks\n    \u2514\u2500\u2500 transformer/  # Transformer model architectures\n</code></pre>"},{"location":"#usage-examples","title":"Usage Examples","text":"<p>Examples and usage documentation will be added as the package develops.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contribution guidelines will be added as the project matures.</p>"},{"location":"#license","title":"License","text":"<p>License information to be determined.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#metrics","title":"Metrics","text":""},{"location":"api/#tfmpe.metrics","title":"<code>tfmpe.metrics</code>","text":"<p>Metrics for analysis of parameter inference.</p>"},{"location":"api/#estimators","title":"Estimators","text":""},{"location":"api/#tfmpe.estimators","title":"<code>tfmpe.estimators</code>","text":"<p>Parameter estimators.</p>"},{"location":"api/#bijectors","title":"Bijectors","text":""},{"location":"api/#tfmpe.bijectors","title":"<code>tfmpe.bijectors</code>","text":"<p>Bijectors for structured parameter sets.</p>"},{"location":"api/#preprocessing","title":"Preprocessing","text":""},{"location":"api/#tfmpe.preprocessing","title":"<code>tfmpe.preprocessing</code>","text":"<p>Pipelines for processing datasets for use with the estimators.</p>"},{"location":"api/#tfmpe.preprocessing.Labeller","title":"<code>Labeller</code>  <code>dataclass</code>","text":"<p>Global label information for a set of keys.</p> <p>Stores key-to-label mappings and provides methods to generate label arrays for token configurations. This decouples labeling from the Tokens class, allowing independent token instances to share consistent label values through a shared Labeller.</p> <p>Attributes:</p> Name Type Description <code>label_map</code> <code>Dict[str, int]</code> <p>Mapping from key names to integer label indices. Used to generate label arrays for token data. All values must be unique (no collisions allowed).</p> Source code in <code>tfmpe/preprocessing/utils.py</code> <pre><code>@dataclass\nclass Labeller:\n    \"\"\"\n    Global label information for a set of keys.\n\n    Stores key-to-label mappings and provides methods to generate\n    label arrays for token configurations. This decouples labeling\n    from the Tokens class, allowing independent token instances to\n    share consistent label values through a shared Labeller.\n\n    Attributes\n    ----------\n    label_map : Dict[str, int]\n        Mapping from key names to integer label indices. Used to\n        generate label arrays for token data. All values must be\n        unique (no collisions allowed).\n    \"\"\"\n\n    label_map: Dict[str, int]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate label_map has no collisions.\"\"\"\n        values = self.label_map.values()\n        if len(values) != len(set(values)):\n            raise ValueError(\"label_map has collision: \"\n                             \"multiple keys map to same label\")\n\n    @property\n    def n_labels(self) -&gt; int:\n        \"\"\"\n        Get number of label classes.\n\n        Returns the count of entries in label_map.\n\n        Returns\n        -------\n        int\n            Number of distinct keys in label_map\n        \"\"\"\n        return len(self.label_map)\n\n    @classmethod\n    def for_keys(cls, keys: List[str]) -&gt; 'Labeller':\n        \"\"\"\n        Create Labeller with sequential labels for keys.\n\n        Assigns sequential integer labels (0, 1, 2, ...) to keys\n        in the order provided.\n\n        Parameters\n        ----------\n        keys : List[str]\n            List of keys to label in order\n\n        Returns\n        -------\n        Labeller\n            Labeller with sequential label assignments\n        \"\"\"\n        label_map = {key: i for i, key in enumerate(keys)}\n        return cls(label_map=label_map)\n\n    def label(self, slices: Dict[str, 'SliceInfo']) -&gt; Array:\n        \"\"\"\n        Generate label array for given token configuration.\n\n        Creates a 1D label array containing the label index for each\n        token, ordered by slices.keys(). Each key contributes a number\n        of labels equal to the total tokens for that key (product of\n        event_shape dimensions).\n\n        Parameters\n        ----------\n        slices : Dict[str, SliceInfo]\n            Token metadata mapping with keys in slice dict order.\n            Contains SliceInfo with event_shape for each key.\n\n        Returns\n        -------\n        Array\n            Label array with shape (n_total_tokens,) containing\n            integer label indices from label_map.\n\n        Raises\n        ------\n        ValueError\n            If label_map is empty\n        KeyError\n            If a key in slices is not in label_map\n        \"\"\"\n        if not self.label_map:\n            raise ValueError(\"Labeller requires at least one key mapping\")\n\n        # Build labels for each key in slices dict order\n        labels_list = []\n        for key in slices.keys():\n            if key not in self.label_map:\n                raise KeyError(f\"Key '{key}' not found in label_map\")\n\n            event_shape = slices[key].event_shape\n            n_tokens = math.prod(event_shape)\n\n            key_labels = jnp.full(n_tokens,\n                                  self.label_map[key],\n                                  dtype=jnp.int32)\n            labels_list.append(key_labels)\n\n        # Concatenate all labels in slice order\n        return jnp.concatenate(labels_list)\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Labeller.n_labels","title":"<code>n_labels</code>  <code>property</code>","text":"<p>Get number of label classes.</p> <p>Returns the count of entries in label_map.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of distinct keys in label_map</p>"},{"location":"api/#tfmpe.preprocessing.Labeller.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate label_map has no collisions.</p> Source code in <code>tfmpe/preprocessing/utils.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate label_map has no collisions.\"\"\"\n    values = self.label_map.values()\n    if len(values) != len(set(values)):\n        raise ValueError(\"label_map has collision: \"\n                         \"multiple keys map to same label\")\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Labeller.for_keys","title":"<code>for_keys(keys)</code>  <code>classmethod</code>","text":"<p>Create Labeller with sequential labels for keys.</p> <p>Assigns sequential integer labels (0, 1, 2, ...) to keys in the order provided.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>List of keys to label in order</p> required <p>Returns:</p> Type Description <code>Labeller</code> <p>Labeller with sequential label assignments</p> Source code in <code>tfmpe/preprocessing/utils.py</code> <pre><code>@classmethod\ndef for_keys(cls, keys: List[str]) -&gt; 'Labeller':\n    \"\"\"\n    Create Labeller with sequential labels for keys.\n\n    Assigns sequential integer labels (0, 1, 2, ...) to keys\n    in the order provided.\n\n    Parameters\n    ----------\n    keys : List[str]\n        List of keys to label in order\n\n    Returns\n    -------\n    Labeller\n        Labeller with sequential label assignments\n    \"\"\"\n    label_map = {key: i for i, key in enumerate(keys)}\n    return cls(label_map=label_map)\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Labeller.label","title":"<code>label(slices)</code>","text":"<p>Generate label array for given token configuration.</p> <p>Creates a 1D label array containing the label index for each token, ordered by slices.keys(). Each key contributes a number of labels equal to the total tokens for that key (product of event_shape dimensions).</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Dict[str, SliceInfo]</code> <p>Token metadata mapping with keys in slice dict order. Contains SliceInfo with event_shape for each key.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Label array with shape (n_total_tokens,) containing integer label indices from label_map.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If label_map is empty</p> <code>KeyError</code> <p>If a key in slices is not in label_map</p> Source code in <code>tfmpe/preprocessing/utils.py</code> <pre><code>def label(self, slices: Dict[str, 'SliceInfo']) -&gt; Array:\n    \"\"\"\n    Generate label array for given token configuration.\n\n    Creates a 1D label array containing the label index for each\n    token, ordered by slices.keys(). Each key contributes a number\n    of labels equal to the total tokens for that key (product of\n    event_shape dimensions).\n\n    Parameters\n    ----------\n    slices : Dict[str, SliceInfo]\n        Token metadata mapping with keys in slice dict order.\n        Contains SliceInfo with event_shape for each key.\n\n    Returns\n    -------\n    Array\n        Label array with shape (n_total_tokens,) containing\n        integer label indices from label_map.\n\n    Raises\n    ------\n    ValueError\n        If label_map is empty\n    KeyError\n        If a key in slices is not in label_map\n    \"\"\"\n    if not self.label_map:\n        raise ValueError(\"Labeller requires at least one key mapping\")\n\n    # Build labels for each key in slices dict order\n    labels_list = []\n    for key in slices.keys():\n        if key not in self.label_map:\n            raise KeyError(f\"Key '{key}' not found in label_map\")\n\n        event_shape = slices[key].event_shape\n        n_tokens = math.prod(event_shape)\n\n        key_labels = jnp.full(n_tokens,\n                              self.label_map[key],\n                              dtype=jnp.int32)\n        labels_list.append(key_labels)\n\n    # Concatenate all labels in slice order\n    return jnp.concatenate(labels_list)\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Tokens","title":"<code>Tokens</code>  <code>dataclass</code>","text":"<p>Unified container for structured token data.</p> <p>Stores all parameters and observations in a single flattened array, with metadata for efficient decoding to structured format.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Array</code> <p>All flattened tokens, shape (*sample_shape, n_total_tokens, max_batch_size)</p> <code>labels</code> <code>Array</code> <p>Integer labels per token, shape (*sample_shape, n_total_tokens)</p> <code>position</code> <code>Array</code> <p>Position index per token within its key group, shape (*sample_shape, n_total_tokens)</p> <code>condition</code> <code>Array</code> <p>Conditioning indicator per token (1.0 for conditioning, 0.0 for target), shape (*sample_shape, n_total_tokens)</p> <code>partition_idx</code> <code>int</code> <p>Static index which separates conditional and target data</p> <code>padding_mask</code> <code>Optional[Array]</code> <p>Padding mask, shape (*sample_shape, n_total_tokens)</p> <code>functional_inputs</code> <code>Optional[Array]</code> <p>Functional inputs for tokens, shape (*sample_shape, n_total_tokens, max_batch_size)</p> Source code in <code>tfmpe/preprocessing/tokens.py</code> <pre><code>@register_pytree_node_class\n@dataclass\nclass Tokens:\n    \"\"\"\n    Unified container for structured token data.\n\n    Stores all parameters and observations in a single flattened array,\n    with metadata for efficient decoding to structured format.\n\n    Attributes\n    ----------\n    data : Array\n        All flattened tokens, shape (*sample_shape, n_total_tokens,\n        max_batch_size)\n    labels : Array\n        Integer labels per token, shape (*sample_shape, n_total_tokens)\n    position : Array\n        Position index per token within its key group,\n        shape (*sample_shape, n_total_tokens)\n    condition : Array\n        Conditioning indicator per token (1.0 for conditioning,\n        0.0 for target), shape (*sample_shape, n_total_tokens)\n    partition_idx: int\n        Static index which separates conditional and target data\n    padding_mask : Optional[Array]\n        Padding mask, shape (*sample_shape, n_total_tokens)\n    functional_inputs : Optional[Array]\n        Functional inputs for tokens, shape (*sample_shape,\n        n_total_tokens, max_batch_size)\n    \"\"\"\n\n    data: Array\n    labels: Array\n    position: Array\n    condition: Array\n    partition_idx: int\n    padding_mask: Optional[Array]\n    functional_inputs: Optional[Array]\n\n    @property\n    def sample_ndims(self) -&gt; int:\n        \"\"\"\n        Get number of sample dimensions from data array.\n\n        Returns\n        -------\n        int\n            Number of leading sample dimensions\n        \"\"\"\n        return len(self.data.shape) - 2  # Remove event and batch\n\n    @property\n    def sample_shape(self) -&gt; Tuple[int, ...]:\n        \"\"\"\n        Get sample shape from data array.\n\n        Returns\n        -------\n        Tuple[int, ...]\n            Shape of sample dimensions\n        \"\"\"\n        return self.data.shape[:self.sample_ndims]\n\n    @classmethod\n    def from_pytree(\n        cls,\n        data: Dict[str, Array],\n        condition: List[str],\n        labeller: Optional[Labeller] = None,\n        functional_inputs: Optional[Dict[str, Array]] = None,\n        sample_ndims: int = 1,\n        batch_ndims: Optional[Dict[str, int]] = None,\n    ) -&gt; 'Tokens':\n        \"\"\"\n        Create Tokens from structured PyTree.\n\n        All keys in data are flattened into a single token array.\n\n        Parameters\n        ----------\n        data: Dict[str, Array]\n            Dictionary of model variable samples. Each array should have shape\n            (*sample_dims, *event_dims, *batch_dims).\n        condition: List[str]\n            List of keys which correspond to conditioning variables\n        labeller : Optional[Labeller], optional\n            Labeller instance for generating consistent labels across tokens.\n            If None, creates a default labeller with sequential indices.\n        functional_inputs : Optional[Dict[str, Array]], optional\n            Dictionary of functional input arrays matching data structure\n        sample_ndims : int, optional\n            Number of leading sample dimensions. Default is 1.\n        batch_ndims : Optional[Dict[str, int]], optional\n            Number of trailing batch dimensions for each key.\n            If None, defaults to 1 for all keys.\n\n        Returns\n        -------\n        Tokens\n            Token object\n        \"\"\"\n        tokens, _ = cls._from_pytree_impl(\n            data, condition, labeller, functional_inputs,\n            sample_ndims, batch_ndims\n        )\n        return tokens\n\n    @classmethod\n    def from_pytree_with_decoder(\n        cls,\n        data: Dict[str, Array],\n        condition: List[str],\n        labeller: Optional[Labeller] = None,\n        functional_inputs: Optional[Dict[str, Array]] = None,\n        sample_ndims: int = 1,\n        batch_ndims: Optional[Dict[str, int]] = None,\n    ) -&gt; Tuple['Tokens', Callable[['Tokens'], Dict[str, Array]]]:\n        \"\"\"\n        Create Tokens from structured PyTree with a decoder function.\n\n        All keys in data are flattened into a single token array.\n\n        Parameters\n        ----------\n        data: Dict[str, Array]\n            Dictionary of model variable samples. Each array should have shape\n            (*sample_dims, *event_dims, *batch_dims).\n        condition: List[str]\n            List of keys which correspond to conditioning variables\n        labeller : Optional[Labeller], optional\n            Labeller instance for generating consistent labels across tokens.\n            If None, creates a default labeller with sequential indices.\n        functional_inputs : Optional[Dict[str, Array]], optional\n            Dictionary of functional input arrays matching data structure\n        sample_ndims : int, optional\n            Number of leading sample dimensions. Default is 1.\n        batch_ndims : Optional[Dict[str, int]], optional\n            Number of trailing batch dimensions for each key.\n            If None, defaults to 1 for all keys.\n\n        Returns\n        -------\n        Tuple[Tokens, Callable[[Tokens], Dict[str, Array]]]\n            Token object and decoding function\n        \"\"\"\n        return cls._from_pytree_impl(\n            data, condition, labeller, functional_inputs,\n            sample_ndims, batch_ndims\n        )\n\n    @classmethod\n    def _from_pytree_impl(\n        cls,\n        data: Dict[str, Array],\n        condition: List[str],\n        labeller: Optional[Labeller],\n        functional_inputs: Optional[Dict[str, Array]],\n        sample_ndims: int,\n        batch_ndims: Optional[Dict[str, int]],\n    ) -&gt; Tuple['Tokens', Callable[['Tokens'], Dict[str, Array]]]:\n        \"\"\"\n        Internal implementation for from_pytree methods.\n\n        Flattens the input PyTree into a token array, generates labels\n        and position indices, and creates a decoder closure that can\n        reconstruct the original structure.\n\n        Parameters\n        ----------\n        data : Dict[str, Array]\n            Dictionary of parameter arrays to flatten\n        condition : List[str]\n            Keys corresponding to conditioning variables (placed first)\n        labeller : Optional[Labeller]\n            Label generator, or None to create default sequential labels\n        functional_inputs : Optional[Dict[str, Array]]\n            Optional functional inputs to flatten alongside data\n        sample_ndims : int\n            Number of leading sample dimensions\n        batch_ndims : Optional[Dict[str, int]]\n            Batch dimensions per key, or None to default to 1\n\n        Returns\n        -------\n        Tuple[Tokens, Callable[[Tokens], Dict[str, Array]]]\n            The constructed Tokens object and a decoder function that\n            reconstructs the original PyTree structure from token data\n        \"\"\"\n        # Default batch_ndims to 1 for all keys\n        if batch_ndims is None:\n            batch_ndims = {key: 1 for key in data.keys()}\n\n        # Create default labeller if not provided\n        if labeller is None:\n            labeller = Labeller.for_keys(list(data.keys()))\n\n        # Flatten the PyTree\n        # Sort data such that conditioning variables come first\n        key_order = sorted(data.keys(), key=lambda k: k not in condition)\n        data = { k: data[k] for k in key_order }\n        flat_data, slices = flatten_pytree(\n            data,\n            sample_ndims=sample_ndims,\n            batch_ndims=batch_ndims\n        )\n        partition_idx = next(\n            s.offset\n            for k, s in slices.items()\n            if k not in condition\n        )\n\n        # Build labels array\n        total_tokens = flat_data.shape[sample_ndims]\n        sample_shape = flat_data.shape[:sample_ndims]\n\n        # Generate labels using Labeller\n        labels_1d = labeller.label(slices)\n        broadcast_shape = sample_shape + (total_tokens,)\n        labels = jnp.broadcast_to(\n            labels_1d.reshape((1,) * sample_ndims + (total_tokens,)),\n            broadcast_shape\n        )\n\n        # Flatten functional inputs if provided\n        func_inputs_flat = None\n        if functional_inputs is not None:\n            func_inputs_flat = flatten_functional_inputs(\n                functional_inputs,\n                slices,\n                sample_ndims=sample_ndims\n            )\n\n        position = jnp.concatenate([\n            jnp.arange(prod(s.event_shape))\n            for s in slices.values()\n        ])\n        position = jnp.broadcast_to(\n            position.reshape((1,) * sample_ndims + (total_tokens,)),\n            broadcast_shape\n        )\n\n        condition_values = jnp.concatenate([\n            jnp.full(\n                (prod(v.event_shape),),\n                int(k in condition),\n                dtype=float\n            )\n            for k, v in slices.items()\n        ])\n        condition_values = jnp.broadcast_to(\n            condition_values.reshape((1,) * sample_ndims + (total_tokens,)),\n            broadcast_shape\n        )\n\n        tokens = cls(\n            data=flat_data,\n            labels=labels,\n            position=position,\n            condition=condition_values,\n            padding_mask=None,\n            functional_inputs=func_inputs_flat,\n            partition_idx=partition_idx\n        )\n\n        def decoder(tokens: 'Tokens') -&gt; Dict[str, Array]:\n            return decode_pytree(\n                tokens.data,\n                slices,\n                tokens.sample_shape,\n                is_subset=False\n            )\n\n        return tokens, decoder\n\n    def tree_flatten(self) -&gt; Tuple[Tuple, Dict[str, Any]]:\n        \"\"\"\n        Flatten Tokens for JAX PyTree operations.\n\n        Returns\n        -------\n        Tuple[Tuple, Dict[str, Any]]\n            (children, aux_data) where children are arrays with sample\n            dimension that get transformed by tree.map, and aux_data\n            contains static metadata\n        \"\"\"\n        children = (\n            self.data,\n            self.labels,\n            self.position,\n            self.condition,\n            self.padding_mask,\n            self.functional_inputs,\n        )\n        aux_data = {\"partition_idx\": self.partition_idx}\n        return (children, aux_data)\n\n    @classmethod\n    def tree_unflatten(\n        cls,\n        aux_data: Dict[str, Any],\n        children: Tuple\n    ) -&gt; 'Tokens':\n        \"\"\"\n        Unflatten Tokens from JAX PyTree operations.\n\n        Parameters\n        ----------\n        aux_data : Dict[str, Any]\n            Static metadata\n        children : Tuple\n            Arrays with sample dimension\n\n        Returns\n        -------\n        Tokens\n            Reconstructed Tokens object\n        \"\"\"\n        (\n            data,\n            labels,\n            position,\n            condition,\n            padding_mask,\n            functional_inputs,\n        ) = children\n        return cls(\n            data=data,\n            labels=labels,\n            position=position,\n            condition=condition,\n            padding_mask=padding_mask,\n            functional_inputs=functional_inputs,\n            partition_idx=aux_data[\"partition_idx\"]\n        )\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Tokens.sample_ndims","title":"<code>sample_ndims</code>  <code>property</code>","text":"<p>Get number of sample dimensions from data array.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of leading sample dimensions</p>"},{"location":"api/#tfmpe.preprocessing.Tokens.sample_shape","title":"<code>sample_shape</code>  <code>property</code>","text":"<p>Get sample shape from data array.</p> <p>Returns:</p> Type Description <code>Tuple[int, ...]</code> <p>Shape of sample dimensions</p>"},{"location":"api/#tfmpe.preprocessing.Tokens.from_pytree","title":"<code>from_pytree(data, condition, labeller=None, functional_inputs=None, sample_ndims=1, batch_ndims=None)</code>  <code>classmethod</code>","text":"<p>Create Tokens from structured PyTree.</p> <p>All keys in data are flattened into a single token array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Array]</code> <p>Dictionary of model variable samples. Each array should have shape (sample_dims, event_dims, *batch_dims).</p> required <code>condition</code> <code>List[str]</code> <p>List of keys which correspond to conditioning variables</p> required <code>labeller</code> <code>Optional[Labeller]</code> <p>Labeller instance for generating consistent labels across tokens. If None, creates a default labeller with sequential indices.</p> <code>None</code> <code>functional_inputs</code> <code>Optional[Dict[str, Array]]</code> <p>Dictionary of functional input arrays matching data structure</p> <code>None</code> <code>sample_ndims</code> <code>int</code> <p>Number of leading sample dimensions. Default is 1.</p> <code>1</code> <code>batch_ndims</code> <code>Optional[Dict[str, int]]</code> <p>Number of trailing batch dimensions for each key. If None, defaults to 1 for all keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tokens</code> <p>Token object</p> Source code in <code>tfmpe/preprocessing/tokens.py</code> <pre><code>@classmethod\ndef from_pytree(\n    cls,\n    data: Dict[str, Array],\n    condition: List[str],\n    labeller: Optional[Labeller] = None,\n    functional_inputs: Optional[Dict[str, Array]] = None,\n    sample_ndims: int = 1,\n    batch_ndims: Optional[Dict[str, int]] = None,\n) -&gt; 'Tokens':\n    \"\"\"\n    Create Tokens from structured PyTree.\n\n    All keys in data are flattened into a single token array.\n\n    Parameters\n    ----------\n    data: Dict[str, Array]\n        Dictionary of model variable samples. Each array should have shape\n        (*sample_dims, *event_dims, *batch_dims).\n    condition: List[str]\n        List of keys which correspond to conditioning variables\n    labeller : Optional[Labeller], optional\n        Labeller instance for generating consistent labels across tokens.\n        If None, creates a default labeller with sequential indices.\n    functional_inputs : Optional[Dict[str, Array]], optional\n        Dictionary of functional input arrays matching data structure\n    sample_ndims : int, optional\n        Number of leading sample dimensions. Default is 1.\n    batch_ndims : Optional[Dict[str, int]], optional\n        Number of trailing batch dimensions for each key.\n        If None, defaults to 1 for all keys.\n\n    Returns\n    -------\n    Tokens\n        Token object\n    \"\"\"\n    tokens, _ = cls._from_pytree_impl(\n        data, condition, labeller, functional_inputs,\n        sample_ndims, batch_ndims\n    )\n    return tokens\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Tokens.from_pytree_with_decoder","title":"<code>from_pytree_with_decoder(data, condition, labeller=None, functional_inputs=None, sample_ndims=1, batch_ndims=None)</code>  <code>classmethod</code>","text":"<p>Create Tokens from structured PyTree with a decoder function.</p> <p>All keys in data are flattened into a single token array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Array]</code> <p>Dictionary of model variable samples. Each array should have shape (sample_dims, event_dims, *batch_dims).</p> required <code>condition</code> <code>List[str]</code> <p>List of keys which correspond to conditioning variables</p> required <code>labeller</code> <code>Optional[Labeller]</code> <p>Labeller instance for generating consistent labels across tokens. If None, creates a default labeller with sequential indices.</p> <code>None</code> <code>functional_inputs</code> <code>Optional[Dict[str, Array]]</code> <p>Dictionary of functional input arrays matching data structure</p> <code>None</code> <code>sample_ndims</code> <code>int</code> <p>Number of leading sample dimensions. Default is 1.</p> <code>1</code> <code>batch_ndims</code> <code>Optional[Dict[str, int]]</code> <p>Number of trailing batch dimensions for each key. If None, defaults to 1 for all keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tokens, Callable[[Tokens], Dict[str, Array]]]</code> <p>Token object and decoding function</p> Source code in <code>tfmpe/preprocessing/tokens.py</code> <pre><code>@classmethod\ndef from_pytree_with_decoder(\n    cls,\n    data: Dict[str, Array],\n    condition: List[str],\n    labeller: Optional[Labeller] = None,\n    functional_inputs: Optional[Dict[str, Array]] = None,\n    sample_ndims: int = 1,\n    batch_ndims: Optional[Dict[str, int]] = None,\n) -&gt; Tuple['Tokens', Callable[['Tokens'], Dict[str, Array]]]:\n    \"\"\"\n    Create Tokens from structured PyTree with a decoder function.\n\n    All keys in data are flattened into a single token array.\n\n    Parameters\n    ----------\n    data: Dict[str, Array]\n        Dictionary of model variable samples. Each array should have shape\n        (*sample_dims, *event_dims, *batch_dims).\n    condition: List[str]\n        List of keys which correspond to conditioning variables\n    labeller : Optional[Labeller], optional\n        Labeller instance for generating consistent labels across tokens.\n        If None, creates a default labeller with sequential indices.\n    functional_inputs : Optional[Dict[str, Array]], optional\n        Dictionary of functional input arrays matching data structure\n    sample_ndims : int, optional\n        Number of leading sample dimensions. Default is 1.\n    batch_ndims : Optional[Dict[str, int]], optional\n        Number of trailing batch dimensions for each key.\n        If None, defaults to 1 for all keys.\n\n    Returns\n    -------\n    Tuple[Tokens, Callable[[Tokens], Dict[str, Array]]]\n        Token object and decoding function\n    \"\"\"\n    return cls._from_pytree_impl(\n        data, condition, labeller, functional_inputs,\n        sample_ndims, batch_ndims\n    )\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Tokens.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten Tokens for JAX PyTree operations.</p> <p>Returns:</p> Type Description <code>Tuple[Tuple, Dict[str, Any]]</code> <p>(children, aux_data) where children are arrays with sample dimension that get transformed by tree.map, and aux_data contains static metadata</p> Source code in <code>tfmpe/preprocessing/tokens.py</code> <pre><code>def tree_flatten(self) -&gt; Tuple[Tuple, Dict[str, Any]]:\n    \"\"\"\n    Flatten Tokens for JAX PyTree operations.\n\n    Returns\n    -------\n    Tuple[Tuple, Dict[str, Any]]\n        (children, aux_data) where children are arrays with sample\n        dimension that get transformed by tree.map, and aux_data\n        contains static metadata\n    \"\"\"\n    children = (\n        self.data,\n        self.labels,\n        self.position,\n        self.condition,\n        self.padding_mask,\n        self.functional_inputs,\n    )\n    aux_data = {\"partition_idx\": self.partition_idx}\n    return (children, aux_data)\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.Tokens.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten Tokens from JAX PyTree operations.</p> <p>Parameters:</p> Name Type Description Default <code>aux_data</code> <code>Dict[str, Any]</code> <p>Static metadata</p> required <code>children</code> <code>Tuple</code> <p>Arrays with sample dimension</p> required <p>Returns:</p> Type Description <code>Tokens</code> <p>Reconstructed Tokens object</p> Source code in <code>tfmpe/preprocessing/tokens.py</code> <pre><code>@classmethod\ndef tree_unflatten(\n    cls,\n    aux_data: Dict[str, Any],\n    children: Tuple\n) -&gt; 'Tokens':\n    \"\"\"\n    Unflatten Tokens from JAX PyTree operations.\n\n    Parameters\n    ----------\n    aux_data : Dict[str, Any]\n        Static metadata\n    children : Tuple\n        Arrays with sample dimension\n\n    Returns\n    -------\n    Tokens\n        Reconstructed Tokens object\n    \"\"\"\n    (\n        data,\n        labels,\n        position,\n        condition,\n        padding_mask,\n        functional_inputs,\n    ) = children\n    return cls(\n        data=data,\n        labels=labels,\n        position=position,\n        condition=condition,\n        padding_mask=padding_mask,\n        functional_inputs=functional_inputs,\n        partition_idx=aux_data[\"partition_idx\"]\n    )\n</code></pre>"},{"location":"api/#tfmpe.preprocessing.combine_tokens","title":"<code>combine_tokens(tokens1, tokens2)</code>","text":"<p>Combine two Tokens objects by concatenating samples and padding tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens1</code> <code>Tokens</code> <p>First Tokens object</p> required <code>tokens2</code> <code>Tokens</code> <p>Second Tokens object</p> required <p>Returns:</p> Type Description <code>Tokens</code> <p>Combined Tokens with concatenated samples and padded tokens</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokens have incompatible functional_inputs or incompatible sample_ndims</p> Notes <ul> <li>Sample dimension (axis 0) is concatenated</li> <li>Token dimension (axis 1) is padded to max across both inputs</li> <li>Padding tokens are applied such that token.partition_idx are consistent</li> <li>Self-attention masks are padded with zeros</li> <li>Padding masks track which tokens are real vs padded</li> </ul> Source code in <code>tfmpe/preprocessing/combine.py</code> <pre><code>def combine_tokens(tokens1: Tokens, tokens2: Tokens) -&gt; Tokens:\n    \"\"\"\n    Combine two Tokens objects by concatenating samples and padding tokens.\n\n    Parameters\n    ----------\n    tokens1 : Tokens\n        First Tokens object\n    tokens2 : Tokens\n        Second Tokens object\n\n    Returns\n    -------\n    Tokens\n        Combined Tokens with concatenated samples and padded tokens\n\n    Raises\n    ------\n    ValueError\n        If tokens have incompatible functional_inputs or\n        incompatible sample_ndims\n\n    Notes\n    -----\n    - Sample dimension (axis 0) is concatenated\n    - Token dimension (axis 1) is padded to max across both inputs\n    - Padding tokens are applied such that token.partition_idx are consistent\n    - Self-attention masks are padded with zeros\n    - Padding masks track which tokens are real vs padded\n    \"\"\"\n    # Check functional_inputs consistency\n    has_func1 = tokens1.functional_inputs is not None\n    has_func2 = tokens2.functional_inputs is not None\n    if has_func1 != has_func2:\n        raise ValueError(\n            \"Cannot combine tokens: one has functional_inputs, \"\n            \"the other does not\"\n        )\n\n    # If both have functional_inputs, check final dimension matches\n    if has_func1 and has_func2:\n        final_dim1 = tokens1.functional_inputs.shape[-1]  # type: ignore\n        final_dim2 = tokens2.functional_inputs.shape[-1]  # type: ignore\n        if final_dim1 != final_dim2:\n            raise ValueError(\n                \"Cannot combine tokens: functional_inputs have different \"\n                f\"final dimensions ({final_dim1} vs {final_dim2})\"\n            )\n\n    if tokens1.sample_ndims != tokens2.sample_ndims:\n        raise ValueError(\n            \"Cannot combine tokens with different sample_ndims:\"\n            f\"sample_ndims ({tokens1.sample_ndims} vs {tokens2.sample_ndims})\"\n        )\n\n    max_n_condition = max(\n        tokens1.partition_idx,\n        tokens2.partition_idx\n    )\n\n    max_n_target = max(\n        tokens1.data.shape[tokens1.sample_ndims] - tokens1.partition_idx,\n        tokens2.data.shape[tokens2.sample_ndims] - tokens2.partition_idx,\n    )\n\n    sample_ndims = tokens1.sample_ndims\n\n    # Add a basic padding mask (use replace to avoid mutating inputs)\n    n_tokens1 = tokens1.data.shape[tokens1.sample_ndims]\n    n_tokens2 = tokens2.data.shape[tokens2.sample_ndims]\n    tokens1 = dataclasses.replace(\n        tokens1,\n        padding_mask=jnp.ones(tokens1.sample_shape + (n_tokens1,))\n    )\n    tokens2 = dataclasses.replace(\n        tokens2,\n        padding_mask=jnp.ones(tokens2.sample_shape + (n_tokens2,))\n    )\n\n    def split_leaf(leaf, idx, sample_ndims):\n        context = lax.slice_in_dim(leaf, 0, idx, axis=sample_ndims)\n        target = lax.slice_in_dim(leaf, idx, leaf.shape[sample_ndims], axis=sample_ndims)\n        return context, target\n\n    def pad_token_leaf(leaf1: Optional[Array], leaf2: Optional[Array]) -&gt; Optional[Array]:\n        if leaf1 is None or leaf2 is None:\n            return None\n        if max_n_condition == 0:\n            return jnp.concatenate([\n                _pad_data_to_max_tokens(leaf1, max_n_target, sample_ndims),\n                _pad_data_to_max_tokens(leaf2, max_n_target, sample_ndims),\n            ], axis=0)\n        context_1, target_1 = split_leaf(leaf1, tokens1.partition_idx, sample_ndims)\n        context_2, target_2 = split_leaf(leaf2, tokens2.partition_idx, sample_ndims)\n        context = jnp.concatenate([\n            _pad_data_to_max_tokens(context_1, max_n_condition, sample_ndims),\n            _pad_data_to_max_tokens(context_2, max_n_condition, sample_ndims)\n        ], axis=0)\n        target = jnp.concatenate([\n            _pad_data_to_max_tokens(target_1, max_n_target, sample_ndims),\n            _pad_data_to_max_tokens(target_2, max_n_target, sample_ndims)\n        ], axis=0)\n        return jnp.concatenate([context, target], axis=sample_ndims)\n\n    combined_tokens = tree.map(\n        pad_token_leaf,\n        tokens1,\n        dataclasses.replace(tokens2, partition_idx = tokens1.partition_idx),\n        is_leaf=lambda x: x is None\n    )\n    combined_tokens.partition_idx = max_n_condition\n    return combined_tokens\n</code></pre>"},{"location":"api/#sampling","title":"Sampling","text":""},{"location":"api/#tfmpe.sampling","title":"<code>tfmpe.sampling</code>","text":"<p>Sampling algorithms for training estimators.</p>"},{"location":"api/#neural-networks","title":"Neural Networks","text":""},{"location":"api/#tfmpe.nn","title":"<code>tfmpe.nn</code>","text":"<p>Neural networks.</p>"},{"location":"api/#transformer","title":"Transformer","text":""},{"location":"api/#tfmpe.nn.transformer","title":"<code>tfmpe.nn.transformer</code>","text":"<p>Transformer model.</p>"},{"location":"api/#tfmpe.nn.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Encoder-only transformer for TFMPE.</p> <p>Encodes all tokens through self-attention encoder blocks, then extracts and projects target tokens to produce a vector field.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>TransformerConfig</code> <p>Configuration for transformer architecture</p> <code>embedding</code> <code>Embedding</code> <p>Embedding layer for token data</p> <code>encoder_blocks</code> <code>Module</code> <p>Vmapped encoder blocks</p> <code>output_linear</code> <code>Linear</code> <p>Linear layer projecting from latent_dim to value_dim</p> Source code in <code>tfmpe/nn/transformer/transformer.py</code> <pre><code>class Transformer(nnx.Module):\n    \"\"\"Encoder-only transformer for TFMPE.\n\n    Encodes all tokens through self-attention encoder blocks, then\n    extracts and projects target tokens to produce a vector field.\n\n    Attributes\n    ----------\n    config : TransformerConfig\n        Configuration for transformer architecture\n    embedding : Embedding\n        Embedding layer for token data\n    encoder_blocks : nnx.Module\n        Vmapped encoder blocks\n    output_linear : nnx.Linear\n        Linear layer projecting from latent_dim to value_dim\n    \"\"\"\n\n    config: TransformerConfig\n    value_dim: int\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        tokens: Tokens,\n        rngs: nnx.Rngs,\n    ) -&gt; None:\n        \"\"\"Initialize transformer.\n\n        Deduces value_dim, n_labels, and functional_inputs_dim from\n        tokens.\n\n        Parameters\n        ----------\n        config : TransformerConfig\n            Configuration containing latent_dim, n_encoder,\n            n_heads, n_ff, label_dim, index_out_dim, dropout,\n            activation\n        tokens : Tokens\n            Full Tokens object containing all data\n        rngs : nnx.Rngs\n            JAX random number generators for parameter\n            initialization\n        \"\"\"\n        self.config = config\n        self.value_dim = tokens.data.shape[-1]\n\n        n_labels = jnp.unique(tokens.labels).shape[0]\n        f_in_in_dim = (\n            tokens.functional_inputs.shape[-1]\n            if tokens.functional_inputs is not None\n            else 0\n        )\n\n        # Create embedding layer\n        self.embedding = Embedding(\n            value_dim=self.value_dim,\n            n_labels=n_labels,\n            label_dim=config.label_dim,\n            pos_dim=config.pos_dim,\n            latent_dim=config.latent_dim,\n            rngs=rngs,\n            f_in_in_dim=f_in_in_dim,\n            f_in_out_dim=config.index_out_dim\n        )\n\n        # Create encoder blocks via vmap\n        @nnx.split_rngs(splits=config.n_encoder)\n        @nnx.vmap(in_axes=(0,), out_axes=0)\n        def create_encoder_block(rngs: nnx.Rngs) -&gt; EncoderBlock:\n            \"\"\"Create a single encoder block.\"\"\"\n            return EncoderBlock(config=config, rngs=rngs)\n\n        self.encoder_blocks = create_encoder_block(rngs)\n\n        # Create output linear layer\n        self.output_linear = nnx.Linear(\n            config.latent_dim,\n            self.value_dim,\n            rngs=rngs,\n        )\n\n    def encode(\n        self,\n        tokens: Tokens,\n        time: Array,\n        deterministic: bool = False,\n    ) -&gt; Array:\n        \"\"\"Encode tokens through encoder blocks.\n\n        Parameters\n        ----------\n        tokens : Tokens\n            Input tokens to encode\n        time : Array\n            Time values, shape (*sample_shape,) or (*sample_shape, 1)\n        deterministic : bool, optional\n            If True, disable dropout. Default is False.\n\n        Returns\n        -------\n        Array\n            Encoded tokens, shape (*sample_shape, n_tokens,\n            latent_dim)\n        \"\"\"\n        # Embed tokens\n        x = self.embedding(tokens, time)\n\n        # Apply encoder blocks sequentially via scan\n        @nnx.scan(in_axes=(nnx.Carry, 0), out_axes=nnx.Carry)\n        def forward(\n            x: Array,\n            encoder_block: EncoderBlock,\n        ) -&gt; Array:\n            \"\"\"Apply a single encoder block and return updated state.\"\"\"\n            x = encoder_block(\n                x,\n                mask=None,\n                deterministic=deterministic,\n            )\n            return x\n\n        x = forward(x, self.encoder_blocks)\n\n        token_dim = len(tokens.sample_shape)\n        return jax.lax.slice_in_dim(\n            x,\n            tokens.partition_idx,\n            None,\n            axis=token_dim\n        )\n\n    def __call__(\n        self,\n        tokens: Tokens,\n        time: Array,\n        deterministic: bool = False,\n    ) -&gt; Array:\n        \"\"\"Forward pass through transformer.\n\n        Parameters\n        ----------\n        tokens: Tokens\n            tokens to encode\n        time : Array\n            Time values, shape (*sample_shape,) or (*sample_shape, 1)\n        deterministic : bool, optional\n            If True, disable dropout. Default is False.\n\n        Returns\n        -------\n        Array\n            Output vector field for target tokens\n        \"\"\"\n        output = self.encode(\n            tokens=tokens,\n            time=time,\n            deterministic=deterministic,\n        )\n\n        return self.output_linear(output)\n</code></pre>"},{"location":"api/#tfmpe.nn.transformer.Transformer.__call__","title":"<code>__call__(tokens, time, deterministic=False)</code>","text":"<p>Forward pass through transformer.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Tokens</code> <p>tokens to encode</p> required <code>time</code> <code>Array</code> <p>Time values, shape (sample_shape,) or (sample_shape, 1)</p> required <code>deterministic</code> <code>bool</code> <p>If True, disable dropout. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Output vector field for target tokens</p> Source code in <code>tfmpe/nn/transformer/transformer.py</code> <pre><code>def __call__(\n    self,\n    tokens: Tokens,\n    time: Array,\n    deterministic: bool = False,\n) -&gt; Array:\n    \"\"\"Forward pass through transformer.\n\n    Parameters\n    ----------\n    tokens: Tokens\n        tokens to encode\n    time : Array\n        Time values, shape (*sample_shape,) or (*sample_shape, 1)\n    deterministic : bool, optional\n        If True, disable dropout. Default is False.\n\n    Returns\n    -------\n    Array\n        Output vector field for target tokens\n    \"\"\"\n    output = self.encode(\n        tokens=tokens,\n        time=time,\n        deterministic=deterministic,\n    )\n\n    return self.output_linear(output)\n</code></pre>"},{"location":"api/#tfmpe.nn.transformer.Transformer.__init__","title":"<code>__init__(config, tokens, rngs)</code>","text":"<p>Initialize transformer.</p> <p>Deduces value_dim, n_labels, and functional_inputs_dim from tokens.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>Configuration containing latent_dim, n_encoder, n_heads, n_ff, label_dim, index_out_dim, dropout, activation</p> required <code>tokens</code> <code>Tokens</code> <p>Full Tokens object containing all data</p> required <code>rngs</code> <code>Rngs</code> <p>JAX random number generators for parameter initialization</p> required Source code in <code>tfmpe/nn/transformer/transformer.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    tokens: Tokens,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    \"\"\"Initialize transformer.\n\n    Deduces value_dim, n_labels, and functional_inputs_dim from\n    tokens.\n\n    Parameters\n    ----------\n    config : TransformerConfig\n        Configuration containing latent_dim, n_encoder,\n        n_heads, n_ff, label_dim, index_out_dim, dropout,\n        activation\n    tokens : Tokens\n        Full Tokens object containing all data\n    rngs : nnx.Rngs\n        JAX random number generators for parameter\n        initialization\n    \"\"\"\n    self.config = config\n    self.value_dim = tokens.data.shape[-1]\n\n    n_labels = jnp.unique(tokens.labels).shape[0]\n    f_in_in_dim = (\n        tokens.functional_inputs.shape[-1]\n        if tokens.functional_inputs is not None\n        else 0\n    )\n\n    # Create embedding layer\n    self.embedding = Embedding(\n        value_dim=self.value_dim,\n        n_labels=n_labels,\n        label_dim=config.label_dim,\n        pos_dim=config.pos_dim,\n        latent_dim=config.latent_dim,\n        rngs=rngs,\n        f_in_in_dim=f_in_in_dim,\n        f_in_out_dim=config.index_out_dim\n    )\n\n    # Create encoder blocks via vmap\n    @nnx.split_rngs(splits=config.n_encoder)\n    @nnx.vmap(in_axes=(0,), out_axes=0)\n    def create_encoder_block(rngs: nnx.Rngs) -&gt; EncoderBlock:\n        \"\"\"Create a single encoder block.\"\"\"\n        return EncoderBlock(config=config, rngs=rngs)\n\n    self.encoder_blocks = create_encoder_block(rngs)\n\n    # Create output linear layer\n    self.output_linear = nnx.Linear(\n        config.latent_dim,\n        self.value_dim,\n        rngs=rngs,\n    )\n</code></pre>"},{"location":"api/#tfmpe.nn.transformer.Transformer.encode","title":"<code>encode(tokens, time, deterministic=False)</code>","text":"<p>Encode tokens through encoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Tokens</code> <p>Input tokens to encode</p> required <code>time</code> <code>Array</code> <p>Time values, shape (sample_shape,) or (sample_shape, 1)</p> required <code>deterministic</code> <code>bool</code> <p>If True, disable dropout. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Encoded tokens, shape (*sample_shape, n_tokens, latent_dim)</p> Source code in <code>tfmpe/nn/transformer/transformer.py</code> <pre><code>def encode(\n    self,\n    tokens: Tokens,\n    time: Array,\n    deterministic: bool = False,\n) -&gt; Array:\n    \"\"\"Encode tokens through encoder blocks.\n\n    Parameters\n    ----------\n    tokens : Tokens\n        Input tokens to encode\n    time : Array\n        Time values, shape (*sample_shape,) or (*sample_shape, 1)\n    deterministic : bool, optional\n        If True, disable dropout. Default is False.\n\n    Returns\n    -------\n    Array\n        Encoded tokens, shape (*sample_shape, n_tokens,\n        latent_dim)\n    \"\"\"\n    # Embed tokens\n    x = self.embedding(tokens, time)\n\n    # Apply encoder blocks sequentially via scan\n    @nnx.scan(in_axes=(nnx.Carry, 0), out_axes=nnx.Carry)\n    def forward(\n        x: Array,\n        encoder_block: EncoderBlock,\n    ) -&gt; Array:\n        \"\"\"Apply a single encoder block and return updated state.\"\"\"\n        x = encoder_block(\n            x,\n            mask=None,\n            deterministic=deterministic,\n        )\n        return x\n\n    x = forward(x, self.encoder_blocks)\n\n    token_dim = len(tokens.sample_shape)\n    return jax.lax.slice_in_dim(\n        x,\n        tokens.partition_idx,\n        None,\n        axis=token_dim\n    )\n</code></pre>"},{"location":"api/#tfmpe.nn.transformer.TransformerConfig","title":"<code>TransformerConfig</code>  <code>dataclass</code>","text":"<p>Configuration container for transformer architecture parameters.</p> <p>This dataclass holds all configuration parameters needed to initialize a transformer model. It provides sensible defaults and validates that the latent dimension is divisible by the number of attention heads.</p> <p>Attributes:</p> Name Type Description <code>latent_dim</code> <code>int</code> <p>Hidden dimension size for transformer layers</p> <code>n_encoder</code> <code>int</code> <p>Number of encoder transformer blocks</p> <code>n_heads</code> <code>int</code> <p>Number of attention heads in multi-head attention</p> <code>n_ff</code> <code>int</code> <p>Number of sequential feedforward layers in MLP blocks</p> <code>label_dim</code> <code>int</code> <p>Dimension for label embeddings</p> <code>pos_dim</code> <code>int</code> <p>Dimension for positional embeddings</p> <code>index_out_dim</code> <code>int</code> <p>Output dimension for index embeddings</p> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization</p> <code>activation</code> <code>Callable</code> <p>Activation function for feedforward layers (e.g., nnx.relu)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If latent_dim is not divisible by n_heads</p> Source code in <code>tfmpe/nn/transformer/config.py</code> <pre><code>@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration container for transformer architecture parameters.\n\n    This dataclass holds all configuration parameters needed to initialize\n    a transformer model. It provides sensible defaults and validates that\n    the latent dimension is divisible by the number of attention heads.\n\n    Attributes\n    ----------\n    latent_dim : int\n        Hidden dimension size for transformer layers\n    n_encoder : int\n        Number of encoder transformer blocks\n    n_heads : int\n        Number of attention heads in multi-head attention\n    n_ff : int\n        Number of sequential feedforward layers in MLP blocks\n    label_dim : int\n        Dimension for label embeddings\n    pos_dim : int\n        Dimension for positional embeddings\n    index_out_dim : int\n        Output dimension for index embeddings\n    dropout : float\n        Dropout rate for regularization\n    activation : Callable\n        Activation function for feedforward layers (e.g., nnx.relu)\n\n    Raises\n    ------\n    ValueError\n        If latent_dim is not divisible by n_heads\n    \"\"\"\n\n    latent_dim: int = 128\n    n_encoder: int = 2\n    n_heads: int = 4\n    n_ff: int = 2\n    label_dim: int = 32\n    index_out_dim: int = 64\n    pos_dim: int = 8\n    dropout: float = 0.1\n    activation: Callable = field(default_factory=lambda: nnx.relu)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration after initialization.\n\n        Raises\n        ------\n        ValueError\n            If latent_dim is not divisible by n_heads\n        \"\"\"\n        if self.latent_dim % self.n_heads != 0:\n            raise ValueError(\n                f\"latent_dim ({self.latent_dim}) must be divisible by \"\n                f\"n_heads ({self.n_heads})\"\n            )\n</code></pre>"},{"location":"api/#tfmpe.nn.transformer.TransformerConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration after initialization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If latent_dim is not divisible by n_heads</p> Source code in <code>tfmpe/nn/transformer/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration after initialization.\n\n    Raises\n    ------\n    ValueError\n        If latent_dim is not divisible by n_heads\n    \"\"\"\n    if self.latent_dim % self.n_heads != 0:\n        raise ValueError(\n            f\"latent_dim ({self.latent_dim}) must be divisible by \"\n            f\"n_heads ({self.n_heads})\"\n        )\n</code></pre>"}]}